# ğŸ“š GSM8K Prompt Engineering Evaluation

This project benchmarks LLaMA-2-7B-Chat and Qwen-7B-Chat models on the GSM8K dataset using different prompt engineering strategies. It supports 4-bit quantized inference for LLaMA-2 via `bitsandbytes`, and float16 inference for Qwen.

---

## ğŸ“¦ Features

* âœ… Zero-shot, Chain-of-Thought, Few-shot, Prolog-style, and Self-Consistency prompt strategies
* âœ… LLaMA-2 with 4-bit quantization using Hugging Face + `bitsandbytes`
* âœ… Qwen with float16 and safe tokenizer support
* âœ… GSM8K test set auto-downloaded and parsed
* âœ… Evaluation logs + CSV result output

---

## ğŸ“ Directory Structure

```
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ llama2_loader.py         # Loads quantized LLaMA-2 model
â”‚   â””â”€â”€ qwen_loader.py           # Loads float16 Qwen model
â”‚
â”œâ”€â”€ prompts/                    # Prompting strategies
â”‚   â”œâ”€â”€ zero_shot.py             # Zero-shot prompting logic
â”‚   â”œâ”€â”€ cot.py                   # Chain-of-thought prompting logic
â”‚   â”œâ”€â”€ few_shot.py              # Few-shot examples logic
â”‚   â”œâ”€â”€ self_consistency.py      # Self-consistency ensemble logic
â”‚   â””â”€â”€ prolog_style.py          # Prolog-style formal prompting
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ dataset.py              # Loads GSM8K dataset
â”‚   â”œâ”€â”€ evaluation.py           # Evaluation loop and logging
â”‚   â”œâ”€â”€ extraction.py           # Answer number extraction via regex
â”‚
â”œâ”€â”€ run_all.py                  # Runs full evaluation for a given model + N samples
â”œâ”€â”€ llama_test.py               # Quick test script for LLaMA-2
â”œâ”€â”€ qwen_test.py                # Quick test script for Qwen
â”œâ”€â”€ requirements.txt            # All required libraries
```

---

## ğŸš€ Getting Started

### 1. Clone the Repo

```bash
git clone https://github.com/your-org/gsm8k-eval-pipeline.git
cd gsm8k-eval-pipeline
```

### 2. Install Requirements

```bash
pip install -r requirements.txt
```

### 3. Run LLaMA or Qwen Evaluation

```bash
python run_all.py llama 20   # for LLaMA-2 (4-bit)
python run_all.py qwen  20   # for Qwen (float16)
```

---

## ğŸ§ª Test Scripts

```bash
python llama_test.py    # Check LLaMA inference sanity
python qwen_test.py     # Check Qwen output
```

---

## ğŸ§  Requirements Summary

See `requirements.txt` â€” includes:

* transformers, torch, accelerate, bitsandbytes
* pandas, numpy, requests, datasets
* openai (optional)
* sentencepiece, protobuf (for tokenizer safety)

---

## ğŸ“ Output Files

* `results/results_<model>_<n>_<timestamp>.csv` â€“ per-strategy results
* `results/<model>_incorrect_<strategy>.txt` â€“ error logs

---

## ğŸ“Œ License

MIT License

---

## ğŸ¤ Contributors

* Meenatchi Sundari Muthirulappan (Author)
  
---

## ğŸ“¬ Contact

For questions or extensions, open an issue or reach out via GitHub Discussions.
